# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## RULES

For any commits, PR, don't mentioned is generated by CLAUDE.

## Project Overview

Fast-dLLM is a diffusion-based Large Language Model (LLM) inference acceleration framework that supports efficient inference for Dream and LLaDA models. The project implements key-value cache mechanisms and confidence-aware parallel decoding to significantly accelerate diffusion model inference (achieving 2x-11x speedup).

## Architecture

The codebase is organized into two main model implementations:

### LLaDA Model (`llada/`)
- **Core Model**: `llada/model/` contains the LLaDA model implementation with configuration and modeling files
- **Interactive Chat**: `llada/chat.py` - Command-line chat interface
- **Web Demo**: `llada/app.py` - Gradio web interface  
- **Evaluation**: `llada/eval_llada.py` - Evaluation script for benchmarks
- **Generation**: `llada/generate.py` - Text generation utilities

### Dream Model (`dream/`)
- **Core Model**: `dream/model/` contains Dream model implementation with generation utilities
- **Interactive Chat**: `dream/demo_multiturn_chat.py` - Multi-turn chat demo
- **Evaluation**: `dream/eval.py` - Evaluation script for benchmarks

Both models support:
- KV Cache for block-wise decoding
- Dual Cache extension for suffix tokens
- Confidence-aware parallel decoding
- Multiple optimization strategies (prefix cache, parallel generation, dual cache)

## Development Commands

### Installation
```bash
pip install -r requirements.txt
```

### Environment Setup (Required for Evaluation)
```bash
export HF_ALLOW_CODE_EVAL=1
export HF_DATASETS_TRUST_REMOTE_CODE=true
```

### LLaDA Usage

**Interactive Chat:**
```bash
python llada/chat.py --gen_length 128 --steps 128 --block_size 32
```

**Web Demo:**
```bash
cd llada && python app.py
```

**Evaluation (GSM8K):**
```bash
cd llada && bash eval_gsm8k.sh
```

**Evaluation (HumanEval):**
```bash
cd llada && bash eval_humaneval.sh
```

### Dream Usage

**Evaluation (GSM8K):**
```bash
cd dream && bash eval_gsm8k.sh
```

**Evaluation (HumanEval):**
```bash
cd dream && bash eval_humaneval.sh
```

## Key Parameters

### LLaDA Parameters
- `--gen_length`: Maximum generation length
- `--steps`: Number of sampling steps
- `--block_size`: Cache block size
- `--use_cache`: Enable KV cache
- `--dual_cache`: Enable dual cache extension
- `--threshold`: Confidence threshold for parallel decoding

### Dream Parameters
- `--max_new_tokens`: Maximum new tokens to generate
- `--diffusion_steps`: Number of diffusion steps
- `--alg`: Algorithm (entropy or confidence_threshold)
- `--use_cache`: Enable prefix cache
- `--dual_cache`: Enable dual cache
- `--threshold`: Confidence threshold

## Evaluation

Both models support evaluation on:
- **GSM8K**: Math problem solving (5-shot)
- **HumanEval**: Code generation (0-shot)

Post-processing is required for HumanEval results:
```bash
python postprocess_code.py {samples_file.jsonl}
```

## Model Paths
- LLaDA: `GSAI-ML/LLaDA-8B-Instruct`
- Dream: `Dream-org/Dream-v0-Base-7B`